[1mdiff --git a/.gitignore b/.gitignore[m
[1mdeleted file mode 100644[m
[1mindex eaca8d2..0000000[m
[1m--- a/.gitignore[m
[1m+++ /dev/null[m
[36m@@ -1,38 +0,0 @@[m
[31m-ï»¿# Python[m
[31m-__pycache__/[m
[31m-*.py[cod][m
[31m-*.class[m
[31m-*.so[m
[31m-.Python[m
[31m-build/[m
[31m-develop-eggs/[m
[31m-dist/[m
[31m-downloads/[m
[31m-eggs/[m
[31m-.eggs/[m
[31m-lib/[m
[31m-lib64/[m
[31m-parts/[m
[31m-sdist/[m
[31m-var/[m
[31m-wheels/[m
[31m-*.egg-info/[m
[31m-.installed.cfg[m
[31m-*.egg[m
[31m-[m
[31m-# BOS[m
[31m-devel/[m
[31m-logs/[m
[31m-.catkin_workspace[m
[31m-[m
[31m-# Model weights[m
[31m-data/models/[m
[31m-[m
[31m-# IDE files[m
[31m-.idea/[m
[31m-.vscode/[m
[31m-*.swp[m
[31m-*.swo[m
[31m-[m
[31m-# OS specific[m
[31m-.DS_Store[m
[1mdiff --git a/BOS_VLM/__init__.py b/BOS_VLM/__init__.py[m
[1mdeleted file mode 100644[m
[1mindex e02abfc..0000000[m
[1m--- a/BOS_VLM/__init__.py[m
[1m+++ /dev/null[m
[36m@@ -1 +0,0 @@[m
[31m-ï»¿[m
[1mdiff --git a/BOS_VLM/models/__init__.py b/BOS_VLM/models/__init__.py[m
[1mdeleted file mode 100644[m
[1mindex e02abfc..0000000[m
[1m--- a/BOS_VLM/models/__init__.py[m
[1m+++ /dev/null[m
[36m@@ -1 +0,0 @@[m
[31m-ï»¿[m
[1mdiff --git a/BOS_VLM/models/base_model.py b/BOS_VLM/models/base_model.py[m
[1mdeleted file mode 100644[m
[1mindex 4cbf520..0000000[m
[1m--- a/BOS_VLM/models/base_model.py[m
[1m+++ /dev/null[m
[36m@@ -1,151 +0,0 @@[m
[31m-ï»¿"""[m
[31m-Base class for visual language models to be used with BOS.[m
[31m-"""[m
[31m-[m
[31m-from abc import ABC, abstractmethod[m
[31m-from typing import Dict, List, Optional, Tuple, Union[m
[31m-import numpy as np[m
[31m-[m
[31m-[m
[31m-class BaseVLM(ABC):[m
[31m-    """Abstract base class for all visual language models."""[m
[31m-    [m
[31m-    def __init__(self, config: Dict = None):[m
[31m-        """[m
[31m-        Initialize the visual language model.[m
[31m-        [m
[31m-        Args:[m
[31m-            config: Configuration dictionary for the model[m
[31m-        """[m
[31m-        self.config = config or {}[m
[31m-        self.model = None[m
[31m-        self.processor = None[m
[31m-        self.device = self.config.get('device', 'cuda' if self.is_cuda_available() else 'cpu')[m
[31m-        [m
[31m-    @staticmethod[m
[31m-    def is_cuda_available() -> bool:[m
[31m-        """Check if CUDA is available."""[m
[31m-        try:[m
[31m-            import torch[m
[31m-            return torch.cuda.is_available()[m
[31m-        except ImportError:[m
[31m-            return False[m
[31m-    [m
[31m-    @abstractmethod[m
[31m-    def load_model(self) -> None:[m
[31m-        """Load the model and processor."""[m
[31m-        pass[m
[31m-    [m
[31m-    @abstractmethod[m
[31m-    def process_image(self, image: np.ndarray) -> any:[m
[31m-        """[m
[31m-        Process an image for model input.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            [m
[31m-        Returns:[m
[31m-            Processed image in the format required by the model[m
[31m-        """[m
[31m-        pass[m
[31m-    [m
[31m-    @abstractmethod[m
[31m-    def process_text(self, text: str) -> any:[m
[31m-        """[m
[31m-        Process text for model input.[m
[31m-        [m
[31m-        Args:[m
[31m-            text: Input text[m
[31m-            [m
[31m-        Returns:[m
[31m-            Processed text in the format required by the model[m
[31m-        """[m
[31m-        pass[m
[31m-        [m
[31m-    @abstractmethod[m
[31m-    def run_inference(self, [m
[31m-                     image: np.ndarray, [m
[31m-                     text: Optional[str] = None,[m
[31m-                     **kwargs) -> Dict:[m
[31m-        """[m
[31m-        Run inference with the model.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            text: Optional text query[m
[31m-            **kwargs: Additional model-specific parameters[m
[31m-            [m
[31m-        Returns:[m
[31m-            Dictionary containing model outputs[m
[31m-        """[m
[31m-        pass[m
[31m-    [m
[31m-    def describe_image(self, image: np.ndarray) -> str:[m
[31m-        """[m
[31m-        Generate a description of the image.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            [m
[31m-        Returns:[m
[31m-            Text description of the image[m
[31m-        """[m
[31m-        return self.run_inference(image, task="describe")["description"][m
[31m-    [m
[31m-    def answer_question(self, image: np.ndarray, question: str) -> str:[m
[31m-        """[m
[31m-        Answer a question about the image.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            question: Question about the image[m
[31m-            [m
[31m-        Returns:[m
[31m-            Answer to the question[m
[31m-        """[m
[31m-        return self.run_inference(image, text=question, task="vqa")["answer"][m
[31m-    [m
[31m-    def detect_objects(self, [m
[31m-                      image: np.ndarray, [m
[31m-                      queries: List[str] = None) -> Dict[str, List[Tuple[float, float, float, float]]]:[m
[31m-        """[m
[31m-        Detect objects in the image.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            queries: List of object categories to detect[m
[31m-            [m
[31m-        Returns:[m
[31m-            Dictionary mapping object categories to bounding boxes[m
[31m-            Format: {category: [(x1, y1, x2, y2, score), ...]}[m
[31m-        """[m
[31m-        return self.run_inference(image, text=queries, task="detection")["detections"][m
[31m-    [m
[31m-    def embed_image(self, image: np.ndarray) -> np.ndarray:[m
[31m-        """[m
[31m-        Generate an embedding for the image.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            [m
[31m-        Returns:[m
[31m-            Image embedding as numpy array[m
[31m-        """[m
[31m-        return self.run_inference(image, task="embed")["embedding"][m
[31m-    [m
[31m-    def embed_text(self, text: str) -> np.ndarray:[m
[31m-        """[m
[31m-        Generate an embedding for the text.[m
[31m-        [m
[31m-        Args:[m
[31m-            text: Input text[m
[31m-            [m
[31m-        Returns:[m
[31m-            Text embedding as numpy array[m
[31m-        """[m
[31m-        # For models that support text embedding[m
[31m-        if hasattr(self, "text_encoder"):[m
[31m-            return self.run_inference(None, text=text, task="embed_text")["embedding"][m
[31m-        else:[m
[31m-            raise NotImplementedError(f"{self.__class__.__name__} does not support text embedding")[m
[31m-# End of file[m
[1mdiff --git a/BOS_VLM/models/clip_model.py b/BOS_VLM/models/clip_model.py[m
[1mdeleted file mode 100644[m
[1mindex 1c0a35b..0000000[m
[1m--- a/BOS_VLM/models/clip_model.py[m
[1m+++ /dev/null[m
[36m@@ -1,196 +0,0 @@[m
[31m-ï»¿"""[m
[31m-CLIP model implementation for BOS_VLM.[m
[31m-"""[m
[31m-[m
[31m-import numpy as np[m
[31m-from typing import Dict, List, Optional, Tuple, Union[m
[31m-import torch[m
[31m-from PIL import Image[m
[31m-[m
[31m-from BOS_VLM.models.base_model import BaseVLM[m
[31m-[m
[31m-[m
[31m-class CLIPModel(BaseVLM):[m
[31m-    """[m
[31m-    Implementation of CLIP (Contrastive Language-Image Pre-Training) model.[m
[31m-    """[m
[31m-    [m
[31m-    def __init__(self, config: Dict = None):[m
[31m-        """[m
[31m-        Initialize the CLIP model.[m
[31m-        [m
[31m-        Args:[m
[31m-            config: Configuration dictionary with the following options:[m
[31m-                - model_name: Name of the CLIP model (default: "openai/clip-vit-base-patch32")[m
[31m-                - device: Device to run the model on (default: "cuda" if available, else "cpu")[m
[31m-                - cache_dir: Directory to cache the model[m
[31m-        """[m
[31m-        super().__init__(config)[m
[31m-        self.model_name = self.config.get("model_name", "openai/clip-vit-base-patch32")[m
[31m-        self.cache_dir = self.config.get("cache_dir", None)[m
[31m-        self.load_model()[m
[31m-        [m
[31m-    def load_model(self) -> None:[m
[31m-        """Load the CLIP model and processor."""[m
[31m-        try:[m
[31m-            from transformers import CLIPProcessor, CLIPModel[m
[31m-        except ImportError:[m
[31m-            raise ImportError([m
[31m-                "Could not import transformers. "[m
[31m-                "Please install it with pip install transformers."[m
[31m-            )[m
[31m-            [m
[31m-        self.model = CLIPModel.from_pretrained([m
[31m-            self.model_name, [m
[31m-            cache_dir=self.cache_dir[m
[31m-        ).to(self.device)[m
[31m-        [m
[31m-        self.processor = CLIPProcessor.from_pretrained([m
[31m-            self.model_name,[m
[31m-            cache_dir=self.cache_dir[m
[31m-        )[m
[31m-        [m
[31m-    def process_image(self, image: np.ndarray) -> torch.Tensor:[m
[31m-        """[m
[31m-        Process an image for CLIP input.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            [m
[31m-        Returns:[m
[31m-            Processed image as torch.Tensor[m
[31m-        """[m
[31m-        # Convert numpy array to PIL Image[m
[31m-        if isinstance(image, np.ndarray):[m
[31m-            image = Image.fromarray(image.astype('uint8'))[m
[31m-            [m
[31m-        # Process the image[m
[31m-        inputs = self.processor([m
[31m-            images=image,[m
[31m-            return_tensors="pt",[m
[31m-            padding=True[m
[31m-        )[m
[31m-        [m
[31m-        return inputs.pixel_values.to(self.device)[m
[31m-    [m
[31m-    def process_text(self, text: Union[str, List[str]]) -> torch.Tensor:[m
[31m-        """[m
[31m-        Process text for CLIP input.[m
[31m-        [m
[31m-        Args:[m
[31m-            text: Input text or list of texts[m
[31m-            [m
[31m-        Returns:[m
[31m-            Processed text as torch.Tensor[m
[31m-        """[m
[31m-        # Handle both single string and list of strings[m
[31m-        if isinstance(text, str):[m
[31m-            text = [text][m
[31m-            [m
[31m-        # Process the text[m
[31m-        inputs = self.processor([m
[31m-            text=text,[m
[31m-            return_tensors="pt",[m
[31m-            padding=True,[m
[31m-            truncation=True[m
[31m-        )[m
[31m-        [m
[31m-        return inputs.input_ids.to(self.device)[m
[31m-        [m
[31m-    def run_inference(self, [m
[31m-                     image: Optional[np.ndarray] = None, [m
[31m-                     text: Optional[Union[str, List[str]]] = None,[m
[31m-                     task: str = "similarity",[m
[31m-                     **kwargs) -> Dict:[m
[31m-        """[m
[31m-        Run inference with the CLIP model.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            text: Text query or list of text queries[m
[31m-            task: Type of inference to run:[m
[31m-                - "similarity": Compute similarity between image and text[m
[31m-                - "embed": Generate image embedding[m
[31m-                - "embed_text": Generate text embedding[m
[31m-            **kwargs: Additional model-specific parameters[m
[31m-            [m
[31m-        Returns:[m
[31m-            Dictionary containing model outputs[m
[31m-        """[m
[31m-        results = {}[m
[31m-        [m
[31m-        # Put model in evaluation mode[m
[31m-        self.model.eval()[m
[31m-        [m
[31m-        with torch.no_grad():[m
[31m-            if task == "similarity" and image is not None and text is not None:[m
[31m-                # Process inputs[m
[31m-                image_input = self.process_image(image)[m
[31m-                text_input = self.process_text(text)[m
[31m-                [m
[31m-                # Get image and text features[m
[31m-                outputs = self.model(input_ids=text_input, pixel_values=image_input)[m
[31m-                [m
[31m-                # Normalize features[m
[31m-                image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)[m
[31m-                text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)[m
[31m-                [m
[31m-                # Compute similarity scores[m
[31m-                logits_per_image = (100.0 * image_embeds @ text_embeds.T).softmax(dim=-1)[m
[31m-                probs = logits_per_image.cpu().numpy()[m
[31m-                [m
[31m-                results["scores"] = probs[0][m
[31m-                [m
[31m-                # If text is a list, return the most similar text[m
[31m-                if isinstance(text, list):[m
[31m-                    best_idx = probs[0].argmax()[m
[31m-                    results["best_match"] = {[m
[31m-                        "text": text[best_idx],[m
[31m-                        "score": float(probs[0][best_idx])[m
[31m-                    }[m
[31m-            [m
[31m-            elif task == "embed" and image is not None:[m
[31m-                # Process image[m
[31m-                image_input = self.process_image(image)[m
[31m-                [m
[31m-                # Get image features[m
[31m-                outputs = self.model.get_image_features(pixel_values=image_input)[m
[31m-                [m
[31m-                # Normalize features[m
[31m-                image_embeds = outputs / outputs.norm(dim=-1, keepdim=True)[m
[31m-                [m
[31m-                results["embedding"] = image_embeds.cpu().numpy()[0][m
[31m-                [m
[31m-            elif task == "embed_text" and text is not None:[m
[31m-                # Process text[m
[31m-                text_input = self.process_text(text)[m
[31m-                [m
[31m-                # Get text features[m
[31m-                outputs = self.model.get_text_features(input_ids=text_input)[m
[31m-                [m
[31m-                # Normalize features[m
[31m-                text_embeds = outputs / outputs.norm(dim=-1, keepdim=True)[m
[31m-                [m
[31m-                results["embedding"] = text_embeds.cpu().numpy()[0][m
[31m-            [m
[31m-            else:[m
[31m-                raise ValueError([m
[31m-                    f"Invalid task '{task}' or missing required inputs. "[m
[31m-                    f"Task '{task}' requires image={image is not None} and text={text is not None}."[m
[31m-                )[m
[31m-                [m
[31m-        return results[m
[31m-    [m
[31m-    def find_best_match(self, image: np.ndarray, candidates: List[str]) -> Tuple[str, float]:[m
[31m-        """[m
[31m-        Find the text that best matches the image from a list of candidates.[m
[31m-        [m
[31m-        Args:[m
[31m-            image: RGB image as numpy array (H, W, 3)[m
[31m-            candidates: List of text candidates[m
[31m-            [m
[31m-        Returns:[m
[31m-            Tuple of (best matching text, score)[m
[31m-        """[m
[31m-        results = self.run_inference(image, candidates, task="similarity")[m
[31m-        return results["best_match"]["text"], results["best_match"]["score"][m
[1mdiff --git a/BOS_VLM/ros/__init__.py b/BOS_VLM/ros/__init__.py[m
[1mdeleted file mode 100644[m
[1mindex e02abfc..0000000[m
[1m--- a/BOS_VLM/ros/__init__.py[m
[1m+++ /dev/null[m
[36m@@ -1 +0,0 @@[m
[31m-ï»¿[m
[1mdiff --git a/BOS_VLM/utils/__init__.py b/BOS_VLM/utils/__init__.py[m
[1mdeleted file mode 100644[m
[1mindex e02abfc..0000000[m
[1m--- a/BOS_VLM/utils/__init__.py[m
[1m+++ /dev/null[m
[36m@@ -1 +0,0 @@[m
[31m-ï»¿[m
[1mdiff --git a/README.md b/README.md[m
[1mindex 062fe8d..b88eae8 100644[m
[1m--- a/README.md[m
[1m+++ b/README.md[m
[36m@@ -1,125 +1 @@[m
[31m-ï»¿# BOS_VLM: Visual Language Models for Brew Operating System[m
[31m-[m
[31m-A lightweight, easy-to-use package for integrating visual language models with the Brew Operating System.[m
[31m-[m
[31m-## Features[m
[31m-[m
[31m-- Simple interface for multiple visual language models (CLIP, OWL-ViT, etc.)[m
[31m-- BOS topics for image inputs and language outputs[m
[31m-- Support for real-time inference on robot camera feeds[m
[31m-- Modular design for easy extension to new models[m
[31m-- Prebuilt Docker container for hassle-free setup[m
[31m-- Built-in support for common robotics tasks:[m
[31m-  - Visual question answering[m
[31m-  - Object grounding[m
[31m-  - Scene description[m
[31m-  - Visual reasoning[m
[31m-[m
[31m-## Directory Structure[m
[31m-[m
[31m-\\\[m
[31m-BOS_VLM/[m
[31m-â”œâ”€â”€ BOS_VLM/                      # Main package[m
[31m-â”‚   â”œâ”€â”€ __init__.py[m
[31m-â”‚   â”œâ”€â”€ models/                   # Model implementations[m
[31m-â”‚   â”‚   â”œâ”€â”€ __init__.py[m
[31m-â”‚   â”‚   â”œâ”€â”€ base_model.py         # Abstract base class[m
[31m-â”‚   â”‚   â”œâ”€â”€ clip_model.py[m
[31m-â”‚   â”‚   â”œâ”€â”€ owlvit_model.py[m
[31m-â”‚   â”‚   â””â”€â”€ ...[m
[31m-â”‚   â”œâ”€â”€ bos/                      # BOS integration[m
[31m-â”‚   â”‚   â”œâ”€â”€ __init__.py[m
[31m-â”‚   â”‚   â”œâ”€â”€ node.py               # Main BOS node[m
[31m-â”‚   â”‚   â”œâ”€â”€ topics.py             # Topic definitions[m
[31m-â”‚   â”‚   â””â”€â”€ utils.py[m
[31m-â”‚   â””â”€â”€ utils/[m
[31m-â”‚       â”œâ”€â”€ __init__.py[m
[31m-â”‚       â”œâ”€â”€ image_processing.py[m
[31m-â”‚       â””â”€â”€ text_processing.py[m
[31m-â”œâ”€â”€ config/                       # Configuration files[m
[31m-â”‚   â”œâ”€â”€ default.yaml              # Default configuration[m
[31m-â”‚   â””â”€â”€ models/                   # Model-specific configurations[m
[31m-â”‚       â”œâ”€â”€ clip.yaml[m
[31m-â”‚       â””â”€â”€ owlvit.yaml[m
[31m-â”œâ”€â”€ launch/                       # BOS launch files[m
[31m-â”‚   â”œâ”€â”€ vlm_node.launch[m
[31m-â”‚   â””â”€â”€ demo.launch[m
[31m-â”œâ”€â”€ scripts/                      # Utility scripts[m
[31m-â”‚   â”œâ”€â”€ download_models.sh[m
[31m-â”‚   â””â”€â”€ benchmark.py[m
[31m-â”œâ”€â”€ examples/                     # Usage examples[m
[31m-â”‚   â”œâ”€â”€ simple_query.py[m
[31m-â”‚   â”œâ”€â”€ object_detection.py[m
[31m-â”‚   â””â”€â”€ interactive_demo.py[m
[31m-â”œâ”€â”€ tests/                        # Unit and integration tests[m
[31m-â”‚   â”œâ”€â”€ test_models.py[m
[31m-â”‚   â””â”€â”€ test_bos_integration.py[m
[31m-â”œâ”€â”€ docs/                         # Documentation[m
[31m-â”‚   â”œâ”€â”€ installation.md[m
[31m-â”‚   â”œâ”€â”€ models.md[m
[31m-â”‚   â”œâ”€â”€ bos_integration.md[m
[31m-â”‚   â””â”€â”€ examples.md[m
[31m-â”œâ”€â”€ Dockerfile                    # Docker setup[m
[31m-â”œâ”€â”€ setup.py                      # Package setup[m
[31m-â”œâ”€â”€ requirements.txt              # Python dependencies[m
[31m-â”œâ”€â”€ LICENSE                       # Open source license[m
[31m-â””â”€â”€ README.md                     # Main documentation[m
[31m-\\\[m
[31m-[m
[31m-## Installation[m
[31m-[m
[31m-\\\ash[m
[31m-# Clone the repository[m
[31m-git clone https://github.com/your-username/BOS_VLM.git[m
[31m-cd BOS_VLM[m
[31m-[m
[31m-# Install dependencies[m
[31m-pip install -e .[m
[31m-[m
[31m-# Download pre-trained models[m
[31m-./scripts/download_models.sh[m
[31m-[m
[31m-# Build BOS package[m
[31m-cd ~/brew_ws/[m
[31m-make[m
[31m-\\\[m
[31m-[m
[31m-## Quick Start[m
[31m-[m
[31m-### Launch the BOS node[m
[31m-[m
[31m-\\\ash[m
[31m-brew launch BOS_VLM vlm_node.launch model:=clip[m
[31m-\\\[m
[31m-[m
[31m-### Query the model[m
[31m-[m
[31m-\\\python[m
[31m-import brew[m
[31m-from BOS_VLM.msg import VLMQuery, VLMResponse[m
[31m-from sensor_msgs.msg import Image[m
[31m-[m
[31m-# Initialize BOS node[m
[31m-brew.init_node('vlm_client')[m
[31m-[m
[31m-# Set up publishers and subscribers[m
[31m-query_pub = brew.Publisher('/vlm/query', VLMQuery, queue_size=10)[m
[31m-response_sub = brew.Subscriber('/vlm/response', VLMResponse, callback)[m
[31m-[m
[31m-# Create a query[m
[31m-query = VLMQuery()[m
[31m-query.image = current_image  # sensor_msgs/Image[m
[31m-query.text = "What objects can I pick up in this scene?"[m
[31m-query.model = "clip"[m
[31m-[m
[31m-# Send the query[m
[31m-query_pub.publish(query)[m
[31m-\\\[m
[31m-[m
[31m-## License[m
[31m-[m
[31m-This project is licensed under the MIT License - see the LICENSE file for details.[m
[31m-[m
[31m-## Contributing[m
[31m-[m
[31m-Contributions are welcome! Please feel free to submit a Pull Request.[m
[32m+[m[32m# BOS_VLM[m
\ No newline at end of file[m
[1mdiff --git a/requirements.txt b/requirements.txt[m
[1mdeleted file mode 100644[m
[1mindex 3f7dc63..0000000[m
[1m--- a/requirements.txt[m
[1m+++ /dev/null[m
[36m@@ -1,6 +0,0 @@[m
[31m-ï»¿numpy>=1.19.0[m
[31m-opencv-python>=4.5.0[m
[31m-transformers>=4.20.0[m
[31m-torch>=1.10.0[m
[31m-pillow>=8.0.0[m
[31m-pyyaml>=5.1[m
