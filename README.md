# BOS_VLM: Visual Language Models for Brew Operating System

A lightweight, easy-to-use package for integrating visual language models with the Brew Operating System.

## Features

- Simple interface for multiple visual language models (CLIP, OWL-ViT, etc.)
- BOS topics for image inputs and language outputs
- Support for real-time inference on robot camera feeds
- Modular design for easy extension to new models
- Prebuilt Docker container for hassle-free setup
- Built-in support for common robotics tasks:
  - Visual question answering
  - Object grounding
  - Scene description
  - Visual reasoning

## Directory Structure

```
ðŸ“¦ BOS_VLM
 â”£ ðŸ“‚ BOS_VLM                     # Main package
 â”ƒ  â”£ ðŸ“œ __init__.py
 â”ƒ  â”£ ðŸ“‚ models                   # Model implementations
 â”ƒ  â”ƒ  â”£ ðŸ“œ __init__.py
 â”ƒ  â”ƒ  â”£ ðŸ“œ base_model.py         # Abstract base class
 â”ƒ  â”ƒ  â”£ ðŸ“œ clip_model.py
 â”ƒ  â”ƒ  â”£ ðŸ“œ owlvit_model.py
 â”ƒ  â”ƒ  â”— ðŸ“œ ...
 â”ƒ  â”£ ðŸ“‚ bos                      # BOS integration
 â”ƒ  â”ƒ  â”£ ðŸ“œ __init__.py
 â”ƒ  â”ƒ  â”£ ðŸ“œ node.py               # Main BOS node
 â”ƒ  â”ƒ  â”£ ðŸ“œ topics.py             # Topic definitions
 â”ƒ  â”ƒ  â”— ðŸ“œ utils.py
 â”ƒ  â”— ðŸ“‚ utils
 â”ƒ     â”£ ðŸ“œ __init__.py
 â”ƒ     â”£ ðŸ“œ image_processing.py
 â”ƒ     â”— ðŸ“œ text_processing.py
 â”£ ðŸ“‚ config                      # Configuration files
 â”ƒ  â”£ ðŸ“œ default.yaml             # Default configuration
 â”ƒ  â”— ðŸ“‚ models                   # Model-specific configurations
 â”ƒ     â”£ ðŸ“œ clip.yaml
 â”ƒ     â”— ðŸ“œ owlvit.yaml
 â”£ ðŸ“‚ launch                      # BOS launch files
 â”ƒ  â”£ ðŸ“œ vlm_node.launch
 â”ƒ  â”— ðŸ“œ demo.launch
 â”£ ðŸ“‚ scripts                     # Utility scripts
 â”ƒ  â”£ ðŸ“œ download_models.sh
 â”ƒ  â”— ðŸ“œ benchmark.py
 â”£ ðŸ“‚ examples                    # Usage examples
 â”ƒ  â”£ ðŸ“œ simple_query.py
 â”ƒ  â”£ ðŸ“œ object_detection.py
 â”ƒ  â”— ðŸ“œ interactive_demo.py
 â”£ ðŸ“‚ tests                       # Unit and integration tests
 â”ƒ  â”£ ðŸ“œ test_models.py
 â”ƒ  â”— ðŸ“œ test_bos_integration.py
 â”£ ðŸ“‚ docs                        # Documentation
 â”ƒ  â”£ ðŸ“œ installation.md
 â”ƒ  â”£ ðŸ“œ models.md
 â”ƒ  â”£ ðŸ“œ bos_integration.md
 â”ƒ  â”— ðŸ“œ examples.md
 â”£ ðŸ“œ Dockerfile                  # Docker setup
 â”£ ðŸ“œ setup.py                    # Package setup
 â”£ ðŸ“œ requirements.txt            # Python dependencies
 â”£ ðŸ“œ LICENSE                     # Open source license
 â”— ðŸ“œ README.md                   # Main documentation
```

## Installation

```bash
# Clone the repository
git clone https://github.com/homebrewroboticsclub/BOS_VLM.git
cd BOS_VLM

# Install dependencies
pip install -e .

# Download pre-trained models
./scripts/download_models.sh

# Build BOS package
cd ~/brew_ws/
make
```

## Quick Start

### Launch the BOS node

```bash
brew launch BOS_VLM vlm_node.launch model:=clip
```

### Query the model

```python
import brew
from BOS_VLM.msg import VLMQuery, VLMResponse
from sensor_msgs.msg import Image

# Initialize BOS node
brew.init_node('vlm_client')

# Set up publishers and subscribers
query_pub = brew.Publisher('/vlm/query', VLMQuery, queue_size=10)
response_sub = brew.Subscriber('/vlm/response', VLMResponse, callback)

# Create a query
query = VLMQuery()
query.image = current_image  # sensor_msgs/Image
query.text = "What objects can I pick up in this scene?"
query.model = "clip"

# Send the query
query_pub.publish(query)
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.